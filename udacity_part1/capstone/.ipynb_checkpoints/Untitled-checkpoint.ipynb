{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Udacity Self-Driving Car Nanodegree\n",
    "## Capstone Project\n",
    "\n",
    "### Team FormulaSDC\n",
    "| Name                       | Email                    |\n",
    "|:---------------------------|:-------------------------|\n",
    "| Prerit Jaiswal             | prerit.jaiswal@gmail.com |\n",
    "| Anton Varfolomeev          | dizvara@gmail.com        |\n",
    "| Kemal Tepe                 | ketepe@gmail.com         |\n",
    "| Paul Walker                | n43tc3d2rp-u1@yahoo.com  |\n",
    "| Matthias von dem Knesebeck | mail@knesebeck.com       |\n",
    "\n",
    "\n",
    "### Overview\n",
    "Following were the main objectives of this project : \n",
    "\n",
    "* Smoothly follow waypoints in the simulator. \n",
    "* Respect the target top speed. \n",
    "* Stop at traffic lights when needed.\n",
    "* Stop and restart controllers when DBW is disabled/enabled.\n",
    "* Publish throttle, steering, and brake commands at 50 Hz.\n",
    "\n",
    "To achieve these objectives, we implemented a finite state machine consisting of 3 states : (i) `go` state,  (ii) `stop` state, and (iii) `idle` state. In the absence of a traffic light or if the light is green, the state is set to `go` with target speed set to the speed limit while ensuring that the transition from current to target speed is smooth. If a red or yellow traffic light is detected, state is set to `stop` if it is possible to bring the car to halt without exceeding maximum braking. Again, a smooth transition is implemented from current speed to 0. Once the car has come to halt, state is changed to `idle`. The speed in `idle` state is set to zero and the car remains in this state until the light turns green and car goes back to `go` state.  For yaw control, we have used `YawController` already provided while for throttle/braking, we used a proportional controller which takes as input the error in speed. As a final step, low pass filters were applied before publishing the commands to ROS.  \n",
    "\n",
    "### Traffic Light Detection\n",
    "The traffic light detection has been realized with the help of OpenCV Cascade Classifier,\n",
    "based on Viola and Jones (P. Viola and M. J. Jones, “Robust real-time face detection,” International journal of computer vision, vol. 57, no. 2, pp. 137–154, 2004.\n",
    ") framework. Training samples were collected from simulator and rosbag frames with the addition of images from [Bosch dataset](https://hci.iwr.uni-heidelberg.de/node/6132).\n",
    "For data augmentation and cascade classifier training custom OpenCV modification by one of the authors was used (https://github.com/diz-vara/opencv).\n",
    "Once a traffic light has been identified, the bounding box is scaled to a 16x32 pixel image. \n",
    "\n",
    "This image is then supplied to a color detection neural network that was trained with numerous examples \n",
    "from the labeled Bosch Traffic Light Dataset as well as samples from the Udacity simulation track and provided rosbags. \n",
    "This network returns the color with the highest resulting probability identified. \n",
    "The Traffic Light Detector then publishes the traffic light waypoint once at least 3 consecutive frames have been \n",
    "identified with the same signal.\n",
    "\n",
    "### Results \n",
    "\n",
    "Car was able to successfully complete track lap while meeting all the objectives. Here is a video demonstration on simulator track: \n",
    "\n",
    "[![Simulator](http://img.youtube.com/vi/9MybAoVeOkI/0.jpg)](http://www.youtube.com/watch?v=9MybAoVeOkI \"Simulator\")\n",
    "\n",
    "The results are presented in the following image samples from the simulator track. The bounding boxes show the detected colors:\n",
    "\n",
    "#### Detection Result for \"Green\" Traffic Light \n",
    "<img src=\"imgs/screenshot_green.png\" width=\"300\" >\n",
    "\n",
    "#### Detection Result for \"Yellow\" Traffic Light \n",
    "<img src=\"imgs/screenshot_yellow.png\" width=\"300\">\n",
    "\n",
    "#### Detection Result for \"Red\" Traffic Light \n",
    "<img src=\"imgs/screenshot_red.png\" width=\"300\">\n",
    "\n",
    "Detection works with real camera images (from the rosbag provided by Udacity),\n",
    "\n",
    "#### Detection Results for rosbag recordings \n",
    "<img src=\"imgs/detected_screenshot_14.12.20172.png\" width=\"250\" > \n",
    "<img src=\"imgs/detected_screenshot_14.12.2017.png\" width=\"250\" > \n",
    "<img src=\"imgs/detected_screenshot_14.12.20173.png\" width=\"250\" > \n",
    "\n",
    " even in harsh lighting conditions:\n",
    "\n",
    "<img src=\"imgs/detected_screenshot_14.12.2017-h3.png\" width=\"250\" > \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-375ac084d3d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LeNet Architecture\n",
    "\n",
    "HINTS for layers:\n",
    "\n",
    "    Convolutional layers:\n",
    "\n",
    "    tf.nn.conv2d\n",
    "    tf.nn.max_pool\n",
    "\n",
    "    For preparing the convolutional layer output for the\n",
    "    fully connected layers.\n",
    "\n",
    "    tf.contrib.flatten\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "def eval_data(xv, yv):\n",
    "    \"\"\"\n",
    "    Given a dataset as input returns the loss and accuracy.\n",
    "    \"\"\"\n",
    "    # If dataset.num_examples is not divisible by BATCH_SIZE\n",
    "    # the remainder will be discarded.\n",
    "    # Ex: If BATCH_SIZE is 64 and training set has 55000 examples\n",
    "    # steps_per_epoch = 55000 // 64 = 859\n",
    "    # num_examples = 859 * 64 = 54976\n",
    "    #\n",
    "    # So in that case we go over 54976 examples instead of 55000.\n",
    "    steps_per_epoch = np.int(np.floor(xv.shape[0] // BATCH_SIZE))\n",
    "    num_examples = steps_per_epoch * BATCH_SIZE\n",
    "    total_acc, total_loss = 0, 0\n",
    "    sess = tf.get_default_session()\n",
    "    for step in range(steps_per_epoch):\n",
    "        batch_start = step * BATCH_SIZE\n",
    "        bx = xv[batch_start:batch_start + BATCH_SIZE]\n",
    "        by = yv[batch_start:batch_start + BATCH_SIZE]\n",
    "        loss, acc = sess.run([loss_op, accuracy_op], feed_dict={batch_x : bx, batch_y: by, keep_prob: 1.0})\n",
    "        total_acc += (acc * bx.shape[0])\n",
    "        total_loss += (loss * bx.shape[0])\n",
    "    return total_loss/num_examples, total_acc/num_examples\n",
    "\n",
    "\n",
    "#%%\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "x = Xgn_t; #np.float32(X_train);\n",
    "y = Y_t;\n",
    "xval = Xgn_v; #np.float32(X_valid);\n",
    "yval = Y_v;\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')                                           \n",
    "\n",
    "\n",
    "#sigs are 32x32x3\n",
    "batch_x = tf.placeholder(tf.float32, [None,32,16,3], name = 'batch_x')\n",
    "# 32 types\n",
    "batch_y = tf.placeholder(tf.int32, (None), name = 'batch_y')\n",
    "ohy = tf.one_hot(batch_y,4);\n",
    "fc2 = MixNet(batch_x)\n",
    "\n",
    "step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 1e-3\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, step, \n",
    "                                          70, 0.998, staircase=True)\n",
    "\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=fc2, labels=ohy))\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = opt.minimize(loss_op, global_step = step)\n",
    "correct_prediction = tf.equal(tf.argmax(fc2, 1), tf.argmax(ohy, 1))\n",
    "accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "saver = tf.train.Saver();\n",
    "\n",
    "\n",
    "#%%\n",
    "    \n",
    "#config = tf.GPUOptions(per_process_gpu_memory_fraction = 0.7)\n",
    "    \n",
    "config = tf.ConfigProto(\n",
    "   gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.4),\n",
    "   device_count = {'GPU': 1}\n",
    ")\n",
    "    \n",
    "save_file = './mixNetI-1.ckpt'\n",
    "    \n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    steps_per_epoch = np.int32(x.shape[0] // BATCH_SIZE)\n",
    "    num_examples = steps_per_epoch * BATCH_SIZE\n",
    "\n",
    "    idx = np.arange(x.shape[0])\n",
    "    # Train model\n",
    "    loss = 0\n",
    "    for i in range(EPOCHS):\n",
    "        np.random.shuffle(idx)\n",
    "        for step in range(steps_per_epoch):\n",
    "            batch_start = step * BATCH_SIZE\n",
    "            bx = x[idx[batch_start:batch_start + BATCH_SIZE]]\n",
    "            by = y[idx[batch_start:batch_start + BATCH_SIZE]]\n",
    "\n",
    "            _,loss = sess.run([train_op, loss_op], feed_dict={batch_x: bx, batch_y: by, keep_prob: 0.5})\n",
    "            #print (\"Epoch \", \"%4d\" % i, \" ,step \", \"%4d\" % step, \" from \", \"%4d\" % steps_per_epoch, \"\\r\");\n",
    "\n",
    "        val_loss, val_acc = eval_data(xval, yval)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation loss = {:.3f}\".format(val_loss), \"Train loss = {:.5f}\".format(loss))\n",
    "        print(\"Validation accuracy = {:.3f}\".format(val_acc))\n",
    "        print(\"Learning rate\", \"%.9f\" % sess.run(learning_rate))\n",
    "        print()\n",
    "    \n",
    "    print (\"Saving %s\" % save_file);\n",
    "    saver.save(sess,save_file)    \n",
    "\n",
    "    # Evaluate on the test data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
